# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-QeZ3tbQXfgtmT2OeN8PGgubpqo5535A
"""

!pip install yfinance
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM

# Download historical stock data
ticker = 'AAPL'
data = yf.download(ticker, start='2020-01-01', end='2023-01-01')
data = data[['Close']]

# Scale data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Prepare training and testing data
train_size = int(len(scaled_data) * 0.8)
train_data = scaled_data[:train_size]
test_data = scaled_data[train_size:]

# Create datasets for LSTM model
def create_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step), 0])
        Y.append(data[i + time_step, 0])
    return np.array(X), np.array(Y)

time_step = 60
X_train, y_train = create_dataset(train_data, time_step)
X_test, y_test = create_dataset(test_data, time_step)

# Reshape input to be [samples, time steps, features] for LSTM
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(time_step, 1)))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dense(units=25))
model.add(Dense(units=1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, batch_size=1, epochs=1)

# Make predictions
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Transform back to original form
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
y_train = scaler.inverse_transform([y_train])
y_test = scaler.inverse_transform([y_test])

# Plot predictions
plt.figure(figsize=(14,8))
plt.plot(data.index, scaler.inverse_transform(scaled_data), label='True Price')
plt.plot(data.index[time_step:len(train_predict)+time_step], train_predict, label='Train Predict')
plt.plot(data.index[len(train_predict)+(time_step*2)+1:len(scaled_data)-1], test_predict, label='Test Predict')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.show()

!pip install scikit-surprise
import pandas as pd
import numpy as np
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split

# Load MovieLens data
url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'
data_path = 'ml-latest-small.zip'
!wget -q {url} -O {data_path}
!unzip -o {data_path}

# Read the ratings data
ratings = pd.read_csv('ml-latest-small/ratings.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')

# Merge ratings and movies data
data = pd.merge(ratings, movies, on='movieId')

# Display the first few rows
data.head()

# Check for missing values
data.isnull().sum()

# Prepare the data for Surprise
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Split the data into train and test sets
trainset, testset = train_test_split(data, test_size=0.25)

# Train the SVD algorithm
algo = SVD()
algo.fit(trainset)

# Make predictions on the test set
predictions = algo.test(testset)

# Calculate accuracy
accuracy.rmse(predictions)

def get_movie_title(movie_id):
    return movies[movies['movieId'] == movie_id]['title'].values[0]

def get_movie_recommendations(user_id, n=10):
    # Get all movie IDs
    movie_ids = ratings['movieId'].unique()

    # Predict ratings for all movies the user hasn't rated yet
    user_ratings = {movie_id: algo.predict(user_id, movie_id).est for movie_id in movie_ids if not ratings[(ratings['userId'] == user_id) & (ratings['movieId'] == movie_id)].empty}

    # Sort the movies by predicted rating
    recommended_movies = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:n]

    # Return the top N recommendations
    return [get_movie_title(movie_id) for movie_id, _ in recommended_movies]

# Example: Get recommendations for user with ID 1
recommendations = get_movie_recommendations(user_id=1, n=10)
print("Top 10 movie recommendations for User 1:")
for idx, movie in enumerate(recommendations):
    print(f"{idx + 1}. {movie}")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF matrix of movie genres
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['genres'])

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Create a mapping of movie titles to indices
movie_indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()

def get_content_based_recommendations(title, n=10):
    idx = movie_indices[title]

    # Get the pairwise similarity scores
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the indices of the top N similar movies
    sim_scores = sim_scores[1:n+1]
    movie_indices = [i[0] for i in sim_scores]

    # Return the top N similar movies
    return movies['title'].iloc[movie_indices].values

# Example: Get content-based recommendations for a movie
content_recommendations = get_content_based_recommendations(title='Toy Story (1995)', n=10)
print("Top 10 content-based movie recommendations for 'Toy Story (1995)':")
for idx, movie in enumerate(content_recommendations):
    print(f"{idx + 1}. {movie}")

!pip install beautifulsoup4 requests

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Function to scrape user's movie data from Letterboxd
def scrape_letterboxd(username):
    base_url = f'https://letterboxd.com/{username}/films/'
    page = 1
    movies = []

    while True:
        url = f'{base_url}page/{page}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all movies on the page
        film_list = soup.find_all('li', class_='poster-container')

        if not film_list:
            break

        for film in film_list:
            title = film.find('img')['alt']
            watched_date = film.find('meta', itemprop='datePublished')['content'] if film.find('meta', itemprop='datePublished') else 'N/A'
            rating = film.find('span', class_='rating').text if film.find('span', class_='rating') else 'N/A'
            movies.append({'title': title, 'watched_date': watched_date, 'rating': rating})

        page += 1

    return pd.DataFrame(movies)

# Example usage
username = 'zachallen'
movies_df = scrape_letterboxd(username)
print(movies_df.head())

def convert_rating(rating):
    if rating == 'N/A':
        return None
    # Convert star symbols to numerical values
    star_count = rating.count('★')
    if '½' in rating:
        return star_count + 0.5
    return star_count

movies_df['rating'] = movies_df['rating'].apply(convert_rating)

# Convert watched_date to datetime
movies_df['watched_date'] = pd.to_datetime(movies_df['watched_date'], errors='coerce')

# Drop rows with missing ratings
movies_df.dropna(subset=['rating'], inplace=True)

print(movies_df.head())

ratings = pd.read_csv('ml-latest-small/ratings.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')

merged_df = pd.merge(movies_df, movies, left_on='title', right_on='title', how='inner')

new_user_id = ratings['userId'].max() + 1
merged_df['userId'] = new_user_id

merged_df = merged_df[['userId', 'movieId', 'rating']]
combined_df = pd.concat([ratings, merged_df])

merged_df = merged_df[['userId', 'movieId', 'rating']]
combined_df = pd.concat([ratings, merged_df])

# Split the data into train and test sets
trainset, testset = train_test_split(data, test_size=0.25)

# Train the SVD algorithm
algo = SVD()
algo.fit(trainset)

# Make predictions on the test set
predictions = algo.test(testset)

# Calculate accuracy
accuracy.rmse(predictions)

def get_movie_title(movie_id, movies):
    return movies[movies['movieId'] == movie_id]['title'].values[0]

def get_movie_recommendations(user_id, n=10):
    # Get all movie IDs
    movie_ids = ratings['movieId'].unique()

    # Predict ratings for all movies the user hasn't rated yet
    user_ratings = {movie_id: algo.predict(user_id, movie_id).est for movie_id in movie_ids if not ratings[(ratings['userId'] == user_id) & (ratings['movieId'] == movie_id)].empty}

    # Sort the movies by predicted rating
    recommended_movies = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:n]

    # Return the top N recommendations
    return [get_movie_title(movie_id, movies) for movie_id, _ in recommended_movies]

# Get recommendations for the new user
recommendations = get_movie_recommendations(user_id=new_user_id, n=10)
print("Top 10 movie recommendations:")
for idx, movie in enumerate(recommendations):
    print(f"{idx + 1}. {movie}")

!pip install beautifulsoup4 requests scikit-surprise

import requests
from bs4 import BeautifulSoup
import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Function to scrape user's movie data from Letterboxd
def scrape_letterboxd(username):
    base_url = f'https://letterboxd.com/{username}/films/'
    page = 1
    movies = []

    while True:
        url = f'{base_url}page/{page}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all movies on the page
        film_list = soup.find_all('li', class_='poster-container')

        if not film_list:
            break

        for film in film_list:
            title = film.find('img')['alt']
            watched_date = film.find('meta', itemprop='datePublished')['content'] if film.find('meta', itemprop='datePublished') else 'N/A'
            rating = film.find('span', class_='rating').text if film.find('span', class_='rating') else 'N/A'
            movies.append({'title': title, 'watched_date': watched_date, 'rating': rating})

        page += 1

    return pd.DataFrame(movies)

# Specify the Letterboxd username
username = 'zachallen'  # Replace with the actual username
movies_df = scrape_letterboxd(username)
print("Scraped data:")
print(movies_df.head())

# Function to convert ratings
def convert_rating(rating):
    if rating == 'N/A':
        return None
    # Convert star symbols to numerical values
    star_count = rating.count('★')
    if '½' in rating:
        return star_count + 0.5
    return star_count

# Apply the conversion function
movies_df['rating'] = movies_df['rating'].apply(convert_rating)

# Convert watched_date to datetime
movies_df['watched_date'] = pd.to_datetime(movies_df['watched_date'], errors='coerce')

# Drop rows with missing ratings
movies_df.dropna(subset=['rating'], inplace=True)
print("Preprocessed data:")
print(movies_df.head())

# Load the MovieLens dataset
ratings = pd.read_csv('ml-latest-small/ratings.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')

# Merge the user's data with the MovieLens data on movie titles
merged_df = pd.merge(movies_df, movies, left_on='title', right_on='title', how='inner')
print("Merged data:")
print(merged_df.head())

# Assign a unique user ID for the Letterboxd user
new_user_id = ratings['userId'].max() + 1
merged_df['userId'] = new_user_id

# Combine the user's data with the MovieLens data
merged_df = merged_df[['userId', 'movieId', 'rating']]
combined_df = pd.concat([ratings, merged_df])
print("Combined data:")
print(combined_df.tail())

# Prepare the data for Surprise
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(combined_df[['userId', 'movieId', 'rating']], reader)

# Split the data into train and test sets
trainset, testset = train_test_split(data, test_size=0.25)

# Train the SVD algorithm
algo = SVD()
algo.fit(trainset)

# Make predictions on the test set
predictions = algo.test(testset)

# Calculate accuracy
accuracy.rmse(predictions)

def get_movie_title(movie_id, movies):
    return movies[movies['movieId'] == movie_id]['title'].values[0]

def get_movie_recommendations(user_id, n=10):
    # Get all movie IDs
    movie_ids = ratings['movieId'].unique()

    # Predict ratings for all movies the user hasn't rated yet
    user_ratings = {movie_id: algo.predict(user_id, movie_id).est for movie_id in movie_ids if not ratings[(ratings['userId'] == user_id) & (ratings['movieId'] == movie_id)].empty}

    # Sort the movies by predicted rating
    recommended_movies = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:n]

    # Return the top N recommendations
    return [get_movie_title(movie_id, movies) for movie_id, _ in recommended_movies]

# Get recommendations for the new user
recommendations = get_movie_recommendations(user_id=new_user_id, n=10)
print("Top 10 movie recommendations:")
for idx, movie in enumerate(recommendations):
    print(f"{idx + 1}. {movie}")

!pip install beautifulsoup4 requests scikit-surprise

import requests
from bs4 import BeautifulSoup
import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Function to scrape user's movie data from Letterboxd
def scrape_letterboxd(username):
    base_url = f'https://letterboxd.com/{username}/films/'
    page = 1
    movies = []

    while True:
        url = f'{base_url}page/{page}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find all movies on the page
        film_list = soup.find_all('li', class_='poster-container')

        if not film_list:
            break

        for film in film_list:
            title = film.find('img')['alt']
            watched_date = film.find('meta', itemprop='datePublished')['content'] if film.find('meta', itemprop='datePublished') else 'N/A'
            rating = film.find('span', class_='rating').text if film.find('span', class_='rating') else 'N/A'
            movies.append({'title': title, 'watched_date': watched_date, 'rating': rating})

        page += 1

    return pd.DataFrame(movies)

# Specify the Letterboxd username
username = 'zachallen'  # Replace with the actual username
movies_df = scrape_letterboxd(username)
print("Scraped data:")
print(movies_df.head())

# Function to convert ratings
def convert_rating(rating):
    if rating == 'N/A':
        return None
    # Convert star symbols to numerical values
    star_count = rating.count('★')
    if '½' in rating:
        return star_count + 0.5
    return star_count

# Apply the conversion function
movies_df['rating'] = movies_df['rating'].apply(convert_rating)

# Convert watched_date to datetime
movies_df['watched_date'] = pd.to_datetime(movies_df['watched_date'], errors='coerce')

# Drop rows with missing ratings
movies_df.dropna(subset=['rating'], inplace=True)
print("Preprocessed data:")
print(movies_df.head())

# Load the MovieLens dataset
ratings = pd.read_csv('ml-latest-small/ratings.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')

# Merge the user's data with the MovieLens data on movie titles
merged_df = pd.merge(movies_df, movies, left_on='title', right_on='title', how='inner')
print("Merged data:")
print(merged_df.head())

# Assign a unique user ID for the Letterboxd user
new_user_id = ratings['userId'].max() + 1
merged_df['userId'] = new_user_id

# Combine the user's data with the MovieLens data
merged_df = merged_df[['userId', 'movieId', 'rating']]
combined_df = pd.concat([ratings, merged_df])
print("Combined data:")
print(combined_df.tail())

# Prepare the data for Surprise
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(combined_df[['userId', 'movieId', 'rating']], reader)

# Split the data into train and test sets
trainset, testset = train_test_split(data, test_size=0.25)

# Train the SVD algorithm
algo = SVD()
algo.fit(trainset)

# Make predictions on the test set
predictions = algo.test(testset)

# Calculate accuracy
accuracy.rmse(predictions)

def get_movie_title(movie_id, movies):
    return movies[movies['movieId'] == movie_id]['title'].values[0]

def get_movie_recommendations(user_id, n=10):
    # Get all movie IDs
    movie_ids = ratings['movieId'].unique()

    # Predict ratings for all movies the user hasn't rated yet
    user_ratings = {movie_id: algo.predict(user_id, movie_id).est for movie_id in movie_ids if len(ratings[(ratings['userId'] == user_id) & (ratings['movieId'] == movie_id)]) == 0}

    # Print user_ratings for debugging
    print(f"user_ratings for user_id {user_id}: {user_ratings}")

    # Sort the movies by predicted rating
    recommended_movies = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:n]

    # Return the top N recommendations
    return [get_movie_title(movie_id, movies) for movie_id, _ in recommended_movies]

# Get recommendations for the new user
recommendations = get_movie_recommendations(user_id=new_user_id, n=10)
print("Top 10 movie recommendations:")
for idx, movie in enumerate(recommendations):
    print(f"{idx + 1}. {movie}")

from surprise.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_epochs': [20, 30, 40],
    'lr_all': [0.002, 0.005, 0.01],
    'reg_all': [0.02, 0.05, 0.1]
}

# Perform grid search
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(data)

# Print the best score and parameters
print(gs.best_score['rmse'])
print(gs.best_params['rmse'])

# Use the best model
best_algo = gs.best_estimator['rmse']
best_algo.fit(trainset)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Create a TF-IDF matrix of movie genres
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['genres'].fillna(''))

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# Create a mapping of movie titles to indices
movie_indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()

def get_content_based_recommendations(title, n=10):
    idx = movie_indices[title]

    # Get the pairwise similarity scores
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the indices of the top N similar movies
    sim_scores = sim_scores[1:n+1]
    movie_indices = [i[0] for i in sim_scores]

    # Return the top N similar movies
    return movies['title'].iloc[movie_indices].values

# Example: Get content-based recommendations for a movie
content_recommendations = get_content_based_recommendations(title='Toy Story (1995)', n=10)
print("Top 10 content-based movie recommendations for 'Toy Story (1995)':")
for idx, movie in enumerate(content_recommendations):
    print(f"{idx + 1}. {movie}")

# Install necessary packages
!pip install streamlit requests beautifulsoup4 scikit-surprise

import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Function to scrape user's movie data from Letterboxd
def scrape_letterboxd(username):
    base_url = f'https://letterboxd.com/{username}/films/'
    page = 1
    movies = []

    while True:
        url = f'{base_url}page/{page}/'
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        film_list = soup.find_all('li', class_='poster-container')

        if not film_list:
            break

        for film in film_list:
            title = film.find('img')['alt']
            watched_date = film.find('meta', itemprop='datePublished')['content'] if film.find('meta', itemprop='datePublished') else 'N/A'
            rating = film.find('span', class_='rating').text if film.find('span', class_='rating') else 'N/A'
            movies.append({'title': title, 'watched_date': watched_date, 'rating': rating})

        page += 1

    return pd.DataFrame(movies)

# Function to convert ratings
def convert_rating(rating):
    if rating == 'N/A':
        return None
    star_count = rating.count('★')
    if '½' in rating:
        return star_count + 0.5
    return star_count

# Function to get movie recommendations using SVD
def get_movie_recommendations(user_id, n=10):
    movie_ids = ratings['movieId'].unique()
    user_ratings = {movie_id: algo.predict(user_id, movie_id).est for movie_id in movie_ids if len(ratings[(ratings['userId'] == user_id) & (ratings['movieId'] == movie_id)]) == 0}
    recommended_movies = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)[:n]
    return [get_movie_title(movie_id, movies) for movie_id, _ in recommended_movies]

def get_movie_title(movie_id, movies):
    return movies[movies['movieId'] == movie_id]['title'].values[0]

# Function for content-based recommendations
def get_content_based_recommendations(title, n=10):
    idx = movie_indices[title]

    # Get the pairwise similarity scores
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the indices of the top N similar movies
    sim_scores = sim_scores[1:n+1]
    movie_indices_list = [i[0] for i in sim_scores]

    # Return the top N similar movies
    return movies['title'].iloc[movie_indices_list].values

# Streamlit interface
st.title('Movie Recommendation System')

username = st.text_input('Enter your Letterboxd username:')
if st.button('Get Recommendations'):
    if username:
        # Scrape data from Letterboxd
        movies_df = scrape_letterboxd(username)
        movies_df['rating'] = movies_df['rating'].apply(convert_rating)
        movies_df['watched_date'] = pd.to_datetime(movies_df['watched_date'], errors='coerce')
        movies_df.dropna(subset=['rating'], inplace=True)

        # Display scraped movies
        st.write(f"Movies watched by {username}:")
        st.dataframe(movies_df[['title', 'rating', 'watched_date']])

        # Load MovieLens dataset and merge with user data
        ratings = pd.read_csv('ml-latest-small/ratings.csv')
        movies = pd.read_csv('ml-latest-small/movies.csv')
        merged_df = pd.merge(movies_df, movies, left_on='title', right_on='title', how='inner')
        new_user_id = ratings['userId'].max() + 1
        merged_df['userId'] = new_user_id
        merged_df = merged_df[['userId', 'movieId', 'rating']]
        combined_df = pd.concat([ratings, merged_df])

        # Prepare data for SVD
        reader = Reader(rating_scale=(0.5, 5.0))
        data = Dataset.load_from_df(combined_df[['userId', 'movieId', 'rating']], reader)
        trainset, testset = train_test_split(data, test_size=0.25)

        # Train SVD algorithm
        algo = SVD()
        algo.fit(trainset)

        # Get movie recommendations
        recommendations = get_movie_recommendations(new_user_id, n=10)

        st.write("Top 10 movie recommendations:")
        for idx, movie in enumerate(recommendations):
            st.write(f"{idx + 1}. {movie}")

            # Example poster URL (replace with actual URL fetching logic)
            poster_url = "https://via.placeholder.com/150"
            st.image(poster_url, caption=movie)

        # Content-based filtering
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(movies['genres'].fillna(''))
        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
        movie_indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()

        # Example: Get content-based recommendations for a movie
        content_recommendations = get_content_based_recommendations(title='Toy Story (1995)', n=10)
        st.write("Top 10 content-based movie recommendations for 'Toy Story (1995)':")
        for idx, movie in enumerate(content_recommendations):
            st.write(f"{idx + 1}. {movie}")